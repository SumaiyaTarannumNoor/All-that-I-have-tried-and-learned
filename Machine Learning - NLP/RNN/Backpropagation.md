# Backpropagation and RNN Training

## Backpropagation
Backpropagation is a supervised learning algorithm used to train artificial neural networks. It works by calculating the gradient of the loss function with respect to the network's weights, allowing the network to adjust its weights to minimize the error between predicted and actual outputs. In essence, it's the process of propagating error information backward through the network to refine its internal parameters. 

[Reference: Geeks for Geeks - Backpropagation](https://www.geeksforgeeks.org/machine-learning/ml-back-propagation-through-time/)

- **Recurrent Neural Networks (RNNs)** are designed to process sequential data.
- Unlike traditional neural networks, RNN outputs depend not only on the current input but also on previous inputs through a memory element.
- This memory allows RNNs to capture temporal dependencies in data, such as time series or language.
- **Training RNNs** involves a specialized version of backpropagation:
  - Instead of updating weights based only on the current timestep, RNNs consider all previous timesteps (\( t, t-1, t-2, t-3, \ldots \)).
- This training method is called **Backpropagation Through Time (BPTT)**:
  - BPTT extends traditional backpropagation to sequential data by "unfolding" the network over time.
  - Gradients are summed across all relevant time steps.
  - This enables RNNs to learn complex temporal patterns in sequential data.

## RNN Architecture
- At each timestep **t**, the RNN maintains a hidden state St, which acts as the network's memory summarizing information from previous inputs.
- The hidden state St is updated by combining:
  - The current input Xt
  - The previous hidden state St-1
  - An activation function is applied to introduce non-linearity.
- The output Yt is generated by transforming this hidden state.

### Mathematical Representation
- **Hidden State Update:**
  - St = g1(Wx * Xt + Ws * St-1)
    - St: Hidden state (memory) at time t
    - g1: Activation function (e.g., tanh, ReLU)
    - Wx: Weight matrix for inputs
    - Xt: Input at time t
    - Ws: Weight matrix for hidden states
    - St-1: Hidden state (memory) at time t-1
    
- **Output Generation:**
  - Yt = g2(Wy * St)
    - Yt: Output at time t
    - Wy: Weight matrix for outputs
    - g2: Activation function (e.g., softmax, sigmoid)
    - St: Hidden state (memory) at time t

### Key Points
- St summarizes all information from previous timesteps up to t.
- The architecture allows RNNs to process and predict sequential data by maintaining this memory.
- **Weight matrices** (Wx, Ws, Wy) are learned during training and govern how information flows and transforms within the network.

---
![Reference: RNN Architecture](https://media.geeksforgeeks.org/wp-content/uploads/20200330110806/Rnn-full.png)

## Error Function at Time t = 3

To train the network, we measure how far the predicted output Y_t is from the desired output d_t using an error function. We use the squared error to measure the difference between the desired output d_t and actual output Y_t:

**E_t = (d_t - Y_t)²**

At t = 3:

**E_3 = (d_3 - Y_3)²**

This error quantifies the difference between the predicted output and the actual output at time 3.

## Updating Weights Using BPTT

BPTT updates the weights W_y, W_s, W_x to minimize the error by computing gradients. Unlike standard backpropagation, BPTT unfolds the network across time steps, considering how errors at time t depend on all previous states.

We want to adjust the weights W_y, W_s and W_x to minimize the error E_3.

### 1. Adjusting Output Weight W_y

The output weight W_y affects the output directly at time 3. This means we calculate how the error changes as Y_3 changes, then how Y_3 changes with respect to W_y. Updating W_y is straightforward because it only influences the current output.

Using the chain rule:

**∂E_3/∂W_y = ∂E_3/∂Y_3 × ∂Y_3/∂W_y**

Where:
- E_3 depends on Y_3, so we differentiate E_3 w.r.t. Y_3
- Y_3 depends on W_y, so we differentiate Y_3 w.r.t. W_y

![Adjusting Wy](https://media.geeksforgeeks.org/wp-content/uploads/20200330110810/wy.png)

### 2. Adjusting Hidden State Weight W_s

The hidden state weight W_s influences not just the current hidden state but all previous ones because each hidden state depends on the previous one. To update W_s, we must consider how changes to W_s affect all hidden states S_1, S_2, S_3 and consequently the output at time 3.

The gradient for W_s considers all previous hidden states because each hidden state depends on the previous one:

**∂E_3/∂W_s = Σ(i=1 to 3) ∂E_3/∂Y_3 × ∂Y_3/∂S_i × ∂S_i/∂W_s**

Breaking down:
- Start with the error gradient at output Y_3
- Propagate gradients back through all hidden states S_3, S_2, S_1 since they affect Y_3
- Each S_i depends on W_s, so we differentiate accordingly

![Adjusting Ws]([path/to/adjusting_ws_image.png](https://media.geeksforgeeks.org/wp-content/uploads/20200330110807/ws.png))

### 3. Adjusting Input Weight W_x

Similar to W_s, the input weight W_x affects all hidden states because the input at each timestep shapes the hidden state. The process considers how every input in the sequence impacts the hidden states leading to the output at time 3.

**∂E_3/∂W_x = Σ(i=1 to 3) ∂E_3/∂Y_3 × ∂Y_3/∂S_i × ∂S_i/∂W_x**

The process is similar to W_s, accounting for all previous hidden states because inputs at each timestep affect the hidden states.

![Adjusting Wx](https://media.geeksforgeeks.org/wp-content/uploads/20200330110809/wx.png)

## Advantages of Backpropagation Through Time (BPTT)

**Captures Temporal Dependencies:** BPTT allows RNNs to learn relationships across time steps, crucial for sequential data like speech, text and time series.

**Unfolding over Time:** By considering all previous states during training, BPTT helps the model understand how past inputs influence future outputs.

**Foundation for Modern RNNs:** BPTT forms the basis for training advanced architectures such as LSTMs and GRUs, enabling effective learning of long sequences.

**Flexible for Variable Length Sequences:** It can handle input sequences of varying lengths, adapting gradient calculations accordingly.

## Limitations of BPTT

**Vanishing Gradient Problem:** When backpropagating over many time steps, gradients tend to shrink exponentially, making early time steps contribute very little to weight updates. This causes the network to "forget" long-term dependencies.

**Exploding Gradient Problem:** Gradients may also grow uncontrollably large, causing unstable updates and making training difficult.

![BPTT Limitations](path/to/bptt_limitations_image.png)

## Solutions

**Long Short-Term Memory (LSTM):** Special RNN cells designed to maintain information over longer sequences and mitigate vanishing gradients.

**Gradient Clipping:** Limits the magnitude of gradients during backpropagation to prevent explosion by normalizing them when exceeding a threshold.

![BPTT Solutions](path/to/bptt_solutions_image.png)

## Conclusion

In this comprehensive guide, we learned how Backpropagation Through Time (BPTT) enables Recurrent Neural Networks to capture temporal dependencies by updating weights across multiple time steps. We covered the mathematical foundations, detailed weight update procedures, advantages of the approach, common limitations like vanishing and exploding gradients, and practical solutions including LSTMs and gradient clipping techniques.

Understanding BPTT is crucial for working with sequential data and forms the foundation for more advanced RNN architectures used in modern deep learning applications.
