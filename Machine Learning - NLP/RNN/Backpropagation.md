## Backpropagation
Backpropagation is a supervised learning algorithm used to train artificial neural networks. It works by calculating the gradient of the loss function with respect
to the network's weights, allowing the network to adjust its weights to minimize the error between predicted and actual outputs. In essence, it's the process of 
propagating error information backward through the network to refine its internal parameters. 

[Reference: Geeks for Geeks - Backpropagation](https://www.geeksforgeeks.org/machine-learning/ml-back-propagation-through-time/)

- **Recurrent Neural Networks (RNNs)** are designed to process sequential data.
- Unlike traditional neural networks, RNN outputs depend not only on the current input but also on previous inputs through a memory element.
- This memory allows RNNs to capture temporal dependencies in data, such as time series or language.
- **Training RNNs** involves a specialized version of backpropagation:
  - Instead of updating weights based only on the current timestep, RNNs consider all previous timesteps (\( t, t-1, t-2, t-3, \ldots \)).
- This training method is called **Backpropagation Through Time (BPTT)**:
  - BPTT extends traditional backpropagation to sequential data by "unfolding" the network over time.
  - Gradients are summed across all relevant time steps.
  - This enables RNNs to learn complex temporal patterns in sequential data.
 
## RNN Architecture

- At each timestep **t**, the RNN maintains a hidden state \( S_t \), which acts as the networkâ€™s memory summarizing information from previous inputs.
- The hidden state \( S_t \) is updated by combining:
  - The current input \( X_t \)
  - The previous hidden state \( S_{t-1} \)
  - An activation function is applied to introduce non-linearity.
- The output \( Y_t \) is generated by transforming this hidden state.

### Mathematical Representation

- **Hidden State Update:**
  \[
  S_t = g_1(W_x X_t + W_s S_{t-1})
  \]
  - \( S_t \): Hidden state (memory) at time \( t \)
  - \( X_t \): Input at time \( t \)
  - \( W_s \): Weight matrix for hidden states
  - \( W_x \): Weight matrix for inputs
  - \( g_1 \): Activation function (e.g., tanh, ReLU)

- **Output Generation:**
  \[
  Y_t = g_2(W_y S_t)
  \]
  - \( Y_t \): Output at time \( t \)
  - \( W_y \): Weight matrix for outputs
  - \( g_2 \): Activation function (e.g., softmax, sigmoid)

### Key Points

- \( S_t \) summarizes all information from previous timesteps up to \( t \).
- The architecture allows RNNs to process and predict sequential data by maintaining this memory.
- **Weight matrices** (\( W_x, W_s, W_y \)) are learned during training and govern how information flows and transforms within the network.

---

![Reference: RNN Architecture](https://media.geeksforgeeks.org/wp-content/uploads/20200330110806/Rnn-full.png)
