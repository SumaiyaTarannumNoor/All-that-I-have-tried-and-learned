## Jailbreak
Jailbreaking is a type of manipulation technique that a user may use on AI models like GPT-4o, GPT-4o MINI, Sonnet, Haiku and many more to manipulate the general rules for the AI model and lead  
it to generate content that the model **WOULD NOT** generate normally. All AI models have a set of restrictions and rules that they must obey in order to function. The purpose of  
jailbreaking is to bypass all these rules.

### What are some general rules? (Gotta use a bit of common sense here.)
- That particular AI's system details  
- That particular AI's admin details  
- Talking about **Certain Personality** (There was a time when ChatGPT **would not** give a single piece of info about Elon Musk at all. Now it's updated.)  
- Harmful Content  
  - AI models are not supposed to generate harmful content.  
  - In general, they cannot generate content like ideas for killing or bombing.  
- Explicit content  
  - The AIs that we use for our daily general chores are **not allowed** to create **explicit content**  
  - There are separate models that are catered toward this type of use, and using them is not **jailbreaking**
